# -*- coding: utf-8 -*-
"""Infrared.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hVA2QCH6_e_PAPngvK6-RzVOksoq8yot

Setting up Google Colab Environment
"""

# Install required packages
!pip install torch torchvision torchsummary opencv-python matplotlib numpy Pillow

# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import cv2
import matplotlib.pyplot as plt
import numpy as np
import os
from PIL import Image
from torchsummary import summary
import requests
import zipfile
from google.colab import files
from google.colab.patches import cv2_imshow

"""Create Data Loading Classes"""

# Copy the complete UIU-Net implementation from previous response
class REBNCONV(nn.Module):
    def __init__(self, in_ch, out_ch, dirate=1):
        super(REBNCONV, self).__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=dirate)
        self.bn = nn.BatchNorm2d(out_ch)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x

class RSU(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3, depth=5, use_dilation=False, dilation_rates=None):
        super(RSU, self).__init__()
        self.depth = depth

        self.conv_in = REBNCONV(in_ch, out_ch, dirate=1)

        self.encoder = nn.ModuleList()
        for i in range(depth):
            if i == 0:
                self.encoder.append(REBNCONV(out_ch, mid_ch, dirate=1))
            else:
                if use_dilation and dilation_rates is not None and i < len(dilation_rates):
                    dirate = dilation_rates[i]
                else:
                    dirate = 1
                self.encoder.append(REBNCONV(mid_ch * (2**(i-1)), mid_ch * (2**i), dirate=dirate))

        self.decoder = nn.ModuleList()
        for i in range(depth-1, 0, -1):
            if use_dilation and dilation_rates is not None and i < len(dilation_rates):
                dirate = dilation_rates[i]
            else:
                dirate = 1
            self.decoder.append(REBNCONV(mid_ch * (2**i) + mid_ch * (2**(i-1)), mid_ch * (2**(i-1)), dirate=dirate))

        self.conv_out = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        x0 = self.conv_in(x)

        enc_outputs = [x0]
        for i, layer in enumerate(self.encoder):
            if i == 0:
                e = layer(enc_outputs[-1])
            else:
                e = layer(F.max_pool2d(enc_outputs[-1], 2, 2))
            enc_outputs.append(e)

        d = enc_outputs[-1]
        for i, layer in enumerate(self.decoder):
            d = F.interpolate(d, size=enc_outputs[-(i+2)].shape[2:], mode='bilinear', align_corners=True)
            d = torch.cat([d, enc_outputs[-(i+2)]], dim=1)
            d = layer(d)

        d = torch.cat([d, x0], dim=1)
        d = self.conv_out(d)

        return d

class CrossChannelAttention(nn.Module):
    def __init__(self, channels, reduction=4):
        super(CrossChannelAttention, self).__init__()
        self.channels = channels
        self.reduction = reduction

        self.gap = nn.AdaptiveAvgPool2d(1)

        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, h, w = x.size()
        y = self.gap(x).view(b, c)
        y = self.excitation(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class InteractiveCrossAttention(nn.Module):
    def __init__(self, channels, reduction=4):
        super(InteractiveCrossAttention, self).__init__()
        self.channels = channels
        self.reduction = reduction

        self.cross_channel_att = CrossChannelAttention(channels, reduction)

        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )

    def forward(self, high_level, low_level):
        F_cA = self.cross_channel_att(low_level)

        avg_out = torch.mean(F_cA, dim=1, keepdim=True)
        max_out, _ = torch.max(F_cA, dim=1, keepdim=True)
        spatial_att = torch.cat([avg_out, max_out], dim=1)
        spatial_att = self.spatial_attention(spatial_att)

        F_icA = high_level * spatial_att
        output = F_cA + F_icA

        return output

class RM_DS(nn.Module):
    def __init__(self, in_ch=3, out_ch=1, base_ch=64):
        super(RM_DS, self).__init__()

        self.encoder1 = RSU(in_ch, base_ch//2, base_ch, depth=5, use_dilation=False)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.encoder2 = RSU(base_ch, base_ch, base_ch*2, depth=5, use_dilation=False)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.encoder3 = RSU(base_ch*2, base_ch*2, base_ch*4, depth=5, use_dilation=True,
                           dilation_rates=[1, 1, 2, 4, 8])
        self.pool3 = nn.MaxPool2d(2, 2)

        self.encoder4 = RSU(base_ch*4, base_ch*4, base_ch*8, depth=5, use_dilation=True,
                           dilation_rates=[1, 2, 4, 8, 16])
        self.pool4 = nn.MaxPool2d(2, 2)

        self.bridge = RSU(base_ch*8, base_ch*8, base_ch*16, depth=5, use_dilation=True,
                         dilation_rates=[1, 2, 4, 8, 16])

        self.up4 = nn.ConvTranspose2d(base_ch*16, base_ch*8, 2, stride=2)
        self.ica4 = InteractiveCrossAttention(base_ch*8)
        self.decoder4 = RSU(base_ch*16, base_ch*8, base_ch*8, depth=5, use_dilation=True)

        self.up3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 2, stride=2)
        self.ica3 = InteractiveCrossAttention(base_ch*4)
        self.decoder3 = RSU(base_ch*8, base_ch*4, base_ch*4, depth=5, use_dilation=True)

        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 2, stride=2)
        self.ica2 = InteractiveCrossAttention(base_ch*2)
        self.decoder2 = RSU(base_ch*4, base_ch*2, base_ch*2, depth=5, use_dilation=False)

        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch, 2, stride=2)
        self.ica1 = InteractiveCrossAttention(base_ch)
        self.decoder1 = RSU(base_ch*2, base_ch, base_ch, depth=5, use_dilation=False)

        self.out1 = nn.Conv2d(base_ch, out_ch, 1)
        self.out2 = nn.Conv2d(base_ch*2, out_ch, 1)
        self.out3 = nn.Conv2d(base_ch*4, out_ch, 1)
        self.out4 = nn.Conv2d(base_ch*8, out_ch, 1)
        self.out5 = nn.Conv2d(base_ch*16, out_ch, 1)

    def forward(self, x):
        e1 = self.encoder1(x)
        e2 = self.encoder2(self.pool1(e1))
        e3 = self.encoder3(self.pool2(e2))
        e4 = self.encoder4(self.pool3(e3))

        bridge = self.bridge(self.pool4(e4))

        d4 = self.up4(bridge)
        d4 = self.ica4(e4, d4)
        d4 = torch.cat([e4, d4], dim=1)
        d4 = self.decoder4(d4)

        d3 = self.up3(d4)
        d3 = self.ica3(e3, d3)
        d3 = torch.cat([e3, d3], dim=1)
        d3 = self.decoder3(d3)

        d2 = self.up2(d3)
        d2 = self.ica2(e2, d2)
        d2 = torch.cat([e2, d2], dim=1)
        d2 = self.decoder2(d2)

        d1 = self.up1(d2)
        d1 = self.ica1(e1, d1)
        d1 = torch.cat([e1, d1], dim=1)
        d1 = self.decoder1(d1)

        out1 = self.out1(d1)
        out2 = F.interpolate(self.out2(d2), size=x.shape[2:], mode='bilinear', align_corners=True)
        out3 = F.interpolate(self.out3(d3), size=x.shape[2:], mode='bilinear', align_corners=True)
        out4 = F.interpolate(self.out4(d4), size=x.shape[2:], mode='bilinear', align_corners=True)
        out5 = F.interpolate(self.out5(bridge), size=x.shape[2:], mode='bilinear', align_corners=True)

        final_out = out1 + out2 + out3 + out4 + out5
        return torch.sigmoid(final_out)

class UIU_Net(nn.Module):
    def __init__(self, in_ch=3, out_ch=1, base_ch=64):
        super(UIU_Net, self).__init__()
        self.rm_ds = RM_DS(in_ch, out_ch, base_ch)

    def forward(self, x):
        return self.rm_ds(x)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import numpy as np
import os
from PIL import Image

# Fixed UIU-Net Implementation with correct channel dimensions
class REBNCONV(nn.Module):
    def __init__(self, in_ch, out_ch, dirate=1):
        super(REBNCONV, self).__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=dirate)
        self.bn = nn.BatchNorm2d(out_ch)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x

class RSU(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3, depth=5):
        super(RSU, self).__init__()
        self.depth = depth

        # Initial convolution
        self.conv_in = REBNCONV(in_ch, out_ch, dirate=1)

        # Encoder - fixed channel dimensions
        self.encoder = nn.ModuleList()
        current_ch = out_ch
        for i in range(depth):
            if i == 0:
                # First encoder layer
                self.encoder.append(REBNCONV(current_ch, mid_ch, dirate=1))
                current_ch = mid_ch
            else:
                # Subsequent encoder layers with pooling
                self.encoder.append(REBNCONV(current_ch, current_ch * 2, dirate=1))
                current_ch = current_ch * 2

        # Decoder - fixed channel dimensions
        self.decoder = nn.ModuleList()
        for i in range(depth-1):
            # Input channels: current + skip connection from encoder
            in_dec_ch = current_ch + (current_ch // 2)
            out_dec_ch = current_ch // 2
            self.decoder.append(REBNCONV(in_dec_ch, out_dec_ch, dirate=1))
            current_ch = out_dec_ch

        # Final convolution
        self.conv_out = REBNCONV(out_ch + current_ch, out_ch, dirate=1)

    def forward(self, x):
        x0 = self.conv_in(x)

        # Encoder forward
        enc_outputs = [x0]
        current = x0
        for i, layer in enumerate(self.encoder):
            if i == 0:
                current = layer(current)
            else:
                current = layer(F.max_pool2d(current, 2, 2))
            enc_outputs.append(current)

        # Decoder forward
        current = enc_outputs[-1]
        for i, (layer, enc_feat) in enumerate(zip(self.decoder, enc_outputs[-2::-1])):
            current = F.interpolate(current, size=enc_feat.shape[2:], mode='bilinear', align_corners=True)
            current = torch.cat([current, enc_feat], dim=1)
            current = layer(current)

        # Final output with residual connection
        current = torch.cat([current, x0], dim=1)
        output = self.conv_out(current)

        return output

class CrossChannelAttention(nn.Module):
    def __init__(self, channels, reduction=4):
        super(CrossChannelAttention, self).__init__()
        self.channels = channels
        self.reduction = reduction

        self.gap = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, h, w = x.size()
        y = self.gap(x).view(b, c)
        y = self.excitation(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class InteractiveCrossAttention(nn.Module):
    def __init__(self, channels, reduction=4):
        super(InteractiveCrossAttention, self).__init__()
        self.channels = channels
        self.reduction = reduction

        self.cross_channel_att = CrossChannelAttention(channels, reduction)

        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )

    def forward(self, high_level, low_level):
        # Ensure both inputs have the same number of channels
        if high_level.shape[1] != low_level.shape[1]:
            # Use 1x1 convolution to match channels if needed
            if not hasattr(self, 'channel_adjust'):
                self.channel_adjust = nn.Conv2d(high_level.shape[1], low_level.shape[1], 1).to(high_level.device)
            high_level = self.channel_adjust(high_level)

        F_cA = self.cross_channel_att(low_level)

        avg_out = torch.mean(F_cA, dim=1, keepdim=True)
        max_out, _ = torch.max(F_cA, dim=1, keepdim=True)
        spatial_att = torch.cat([avg_out, max_out], dim=1)
        spatial_att = self.spatial_attention(spatial_att)

        F_icA = high_level * spatial_att
        output = F_cA + F_icA

        return output

class SimplifiedUIU_Net(nn.Module):
    """Simplified UIU-Net with fixed channel dimensions"""
    def __init__(self, in_channels=3, out_channels=1, base_channels=32):
        super(SimplifiedUIU_Net, self).__init__()

        # Encoder
        self.enc1 = RSU(in_channels, base_channels//2, base_channels, depth=3)
        self.pool1 = nn.MaxPool2d(2)

        self.enc2 = RSU(base_channels, base_channels, base_channels*2, depth=3)
        self.pool2 = nn.MaxPool2d(2)

        self.enc3 = RSU(base_channels*2, base_channels*2, base_channels*4, depth=3)
        self.pool3 = nn.MaxPool2d(2)

        # Bridge
        self.bridge = RSU(base_channels*4, base_channels*4, base_channels*8, depth=3)

        # Decoder
        self.up3 = nn.ConvTranspose2d(base_channels*8, base_channels*4, 2, stride=2)
        self.ica3 = InteractiveCrossAttention(base_channels*4)
        self.dec3 = RSU(base_channels*8, base_channels*4, base_channels*4, depth=3)

        self.up2 = nn.ConvTranspose2d(base_channels*4, base_channels*2, 2, stride=2)
        self.ica2 = InteractiveCrossAttention(base_channels*2)
        self.dec2 = RSU(base_channels*4, base_channels*2, base_channels*2, depth=3)

        self.up1 = nn.ConvTranspose2d(base_channels*2, base_channels, 2, stride=2)
        self.ica1 = InteractiveCrossAttention(base_channels)
        self.dec1 = RSU(base_channels*2, base_channels, base_channels, depth=3)

        # Output
        self.out_conv = nn.Conv2d(base_channels, out_channels, 1)

    def forward(self, x):
        # Encoder
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        e3 = self.enc3(self.pool2(e2))

        # Bridge
        b = self.bridge(self.pool3(e3))

        # Decoder with IC-A
        d3 = self.up3(b)
        d3 = self.ica3(e3, d3)
        d3 = torch.cat([e3, d3], dim=1)
        d3 = self.dec3(d3)

        d2 = self.up2(d3)
        d2 = self.ica2(e2, d2)
        d2 = torch.cat([e2, d2], dim=1)
        d2 = self.dec2(d2)

        d1 = self.up1(d2)
        d1 = self.ica1(e1, d1)
        d1 = torch.cat([e1, d1], dim=1)
        d1 = self.dec1(d1)

        # Output
        out = self.out_conv(d1)
        return torch.sigmoid(out)

# Data generation
class SyntheticInfraredDataset(Dataset):
    def __init__(self, num_samples=100, image_size=(256, 256)):
        self.num_samples = num_samples
        self.image_size = image_size

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # Generate synthetic infrared-like image
        image = np.random.rand(3, self.image_size[0], self.image_size[1]).astype(np.float32) * 0.3

        # Generate synthetic small targets
        mask = np.zeros((1, self.image_size[0], self.image_size[1]), dtype=np.float32)

        # Add random small targets
        num_targets = np.random.randint(1, 4)
        for _ in range(num_targets):
            x = np.random.randint(10, self.image_size[0]-10)
            y = np.random.randint(10, self.image_size[1]-10)
            size = np.random.randint(2, 6)

            # Create Gaussian-like target
            for i in range(max(0, x-size), min(self.image_size[0], x+size)):
                for j in range(max(0, y-size), min(self.image_size[1], y+size)):
                    dist = np.sqrt((i-x)**2 + (j-y)**2)
                    if dist < size:
                        mask[0, i, j] = max(mask[0, i, j], np.exp(-dist**2/(2*(size/2)**2)))

        # Add target signals to image
        for c in range(3):
            image[c] += mask[0] * 0.7

        image = np.clip(image, 0, 1)

        return torch.from_numpy(image), torch.from_numpy(mask)

# Fixed training function
def train_fixed_model():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Create fixed model
    model = SimplifiedUIU_Net(in_channels=3, out_channels=1, base_channels=32)
    model = model.to(device)

    # Print model architecture
    print("Model created successfully!")

    # Test forward pass
    test_input = torch.randn(2, 3, 256, 256).to(device)
    try:
        test_output = model(test_input)
        print(f"Test forward pass successful!")
        print(f"Input shape: {test_input.shape}")
        print(f"Output shape: {test_output.shape}")
    except Exception as e:
        print(f"Error in forward pass: {e}")
        return None

    # Create small dataset
    train_dataset = SyntheticInfraredDataset(num_samples=50, image_size=(256, 256))
    val_dataset = SyntheticInfraredDataset(num_samples=10, image_size=(256, 256))

    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)

    # Loss and optimizer
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    # Training loop
    num_epochs = 5
    train_losses = []

    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0.0

        for batch_idx, (images, masks) in enumerate(train_loader):
            images, masks = images.to(device), masks.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

            if batch_idx % 10 == 0:
                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.6f}')

        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)
        print(f'Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.6f}')
        print('-' * 50)

    # Plot training history
    plt.figure(figsize=(8, 4))
    plt.plot(train_losses, label='Training Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training History')
    plt.show()

    return model, train_losses

# Test the fixed model
print("Testing fixed UIU-Net implementation...")
trained_model, losses = train_fixed_model()

if trained_model is not None:
    print("Training completed successfully!")

    # Test with a sample
    device = next(trained_model.parameters()).device
    test_dataset = SyntheticInfraredDataset(num_samples=3, image_size=(256, 256))
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)

    trained_model.eval()
    with torch.no_grad():
        for i, (images, masks) in enumerate(test_loader):
            images, masks = images.to(device), masks.to(device)
            outputs = trained_model(images)

            # Convert to numpy for visualization
            image_np = images[0].cpu().permute(1, 2, 0).numpy()
            mask_np = masks[0][0].cpu().numpy()
            output_np = outputs[0][0].cpu().numpy()

            # Create visualization
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            axes[0].imshow(image_np[:, :, 0], cmap='gray')
            axes[0].set_title('Input Image')
            axes[0].axis('off')

            axes[1].imshow(mask_np, cmap='hot')
            axes[1].set_title('Ground Truth')
            axes[1].axis('off')

            axes[2].imshow(output_np, cmap='hot')
            axes[2].set_title('Prediction')
            axes[2].axis('off')

            plt.tight_layout()
            plt.show()

            if i >= 2:  # Show only 3 samples
                break
else:
    print("Training failed. Please check the model implementation.")